{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 748659,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 571749,
          "modelId": 584072
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***INSTRUCTION FINE-TUNING***\n",
        "\n",
        "**STEP 1: PREPARING DATASET**\n",
        "\n",
        "The dataset consists of 1100 instruction-response pairs."
      ],
      "metadata": {
        "id": "jX1dDOt7D_8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ],
      "metadata": {
        "trusted": true,
        "id": "oM_qy5k9D_8H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data list , which we loaded from the JSON file contains the 1100 entries of the instruction dataset."
      ],
      "metadata": {
        "id": "XQRDhKNkD_8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", data[50])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:03.288684Z",
          "iopub.execute_input": "2026-02-12T15:13:03.288983Z",
          "iopub.status.idle": "2026-02-12T15:13:03.293562Z",
          "shell.execute_reply.started": "2026-02-12T15:13:03.288958Z",
          "shell.execute_reply": "2026-02-12T15:13:03.293023Z"
        },
        "id": "t1WQ4uk2D_8L",
        "outputId": "d4d6fda6-ce49-4cbd-a17d-4a51bd2ba974"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Example entry:\n {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Another example entry:\\n\", data[999])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:06.398273Z",
          "iopub.execute_input": "2026-02-12T15:13:06.399016Z",
          "iopub.status.idle": "2026-02-12T15:13:06.403083Z",
          "shell.execute_reply.started": "2026-02-12T15:13:06.398984Z",
          "shell.execute_reply": "2026-02-12T15:13:06.402364Z"
        },
        "id": "GZuzITxGD_8Q",
        "outputId": "40a0cbc8-2f7e-49b0-8824-a44783481dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Another example entry:\n {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONVERTING INSTRUCTIONS INTO ALPACA FORMAT:**"
      ],
      "metadata": {
        "id": "HuyQqs7MD_8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:08.931628Z",
          "iopub.execute_input": "2026-02-12T15:13:08.931952Z",
          "iopub.status.idle": "2026-02-12T15:13:08.936276Z",
          "shell.execute_reply.started": "2026-02-12T15:13:08.931926Z",
          "shell.execute_reply": "2026-02-12T15:13:08.935697Z"
        },
        "id": "riaVay_bD_8T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This format_input function takes a dictionary entry as input and constructs a formatted string."
      ],
      "metadata": {
        "id": "4IsOiDDvD_8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:12.861523Z",
          "iopub.execute_input": "2026-02-12T15:13:12.862266Z",
          "iopub.status.idle": "2026-02-12T15:13:12.866504Z",
          "shell.execute_reply.started": "2026-02-12T15:13:12.862237Z",
          "shell.execute_reply": "2026-02-12T15:13:12.865716Z"
        },
        "id": "n2v7V7_MD_8W",
        "outputId": "ab38da25-647a-4aa0-cc9a-564d1be7550b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nIdentify the correct spelling of the following word.\n\n### Input:\nOcassion\n\n### Response:\nThe correct spelling is 'Occasion.'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the format_input skips the optional ### Input: section if the 'input' field is empty, which we can test out by applying the format_input function to entry data[999] that we inspected earlier:"
      ],
      "metadata": {
        "id": "8dw9M3ydD_8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[999])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:16.892036Z",
          "iopub.execute_input": "2026-02-12T15:13:16.892803Z",
          "iopub.status.idle": "2026-02-12T15:13:16.896796Z",
          "shell.execute_reply.started": "2026-02-12T15:13:16.892765Z",
          "shell.execute_reply": "2026-02-12T15:13:16.896065Z"
        },
        "id": "z8EpdRuwD_8Y",
        "outputId": "2ee1dfc3-c02f-4d15-d809-a0de909a0ea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat is an antonym of 'complicated'?\n\n### Response:\nAn antonym of 'complicated' is 'simple'.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPLITTING DATASET INTO TRAIN-TEST-VALIDATION:**"
      ],
      "metadata": {
        "id": "qI8iOUPMD_8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:21.251575Z",
          "iopub.execute_input": "2026-02-12T15:13:21.251896Z",
          "iopub.status.idle": "2026-02-12T15:13:21.256659Z",
          "shell.execute_reply.started": "2026-02-12T15:13:21.251869Z",
          "shell.execute_reply": "2026-02-12T15:13:21.255952Z"
        },
        "id": "wLTy3NS-D_8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:24.361938Z",
          "iopub.execute_input": "2026-02-12T15:13:24.362484Z",
          "iopub.status.idle": "2026-02-12T15:13:24.366866Z",
          "shell.execute_reply.started": "2026-02-12T15:13:24.362458Z",
          "shell.execute_reply": "2026-02-12T15:13:24.366154Z"
        },
        "id": "8KCTVqu3D_8b",
        "outputId": "cff88880-0a8a-46f6-937e-91a9b2a1edfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training set length: 935\nValidation set length: 55\nTest set length: 110\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2: ORGANIZING DATA INTO TRAINING BATCHES**\n",
        "\n"
      ],
      "metadata": {
        "id": "SVXU40ZvD_8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenizing texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:27.292249Z",
          "iopub.execute_input": "2026-02-12T15:13:27.292575Z",
          "iopub.status.idle": "2026-02-12T15:13:30.507908Z",
          "shell.execute_reply.started": "2026-02-12T15:13:27.292548Z",
          "shell.execute_reply": "2026-02-12T15:13:30.507288Z"
        },
        "id": "ijB8FCP-D_8e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aim to accelerate training by collecting multiple training examples in a batch, which necessitates padding all inputs to a similar length.\n",
        "\n",
        "We use the <|endoftext|> token as a padding token.\n",
        "\n",
        "Instead of appending the <|endoftext|> tokens to the text inputs, we can append its token ID to the pre-tokenized inputs directly."
      ],
      "metadata": {
        "id": "at3UIiFfD_8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:34.841903Z",
          "iopub.execute_input": "2026-02-12T15:13:34.842301Z",
          "iopub.status.idle": "2026-02-12T15:13:35.85182Z",
          "shell.execute_reply.started": "2026-02-12T15:13:34.842275Z",
          "shell.execute_reply": "2026-02-12T15:13:35.851097Z"
        },
        "id": "iJidK5KpD_8g",
        "outputId": "b37d6ef8-1589-495f-84f5-2e734047f82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[50256]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We padded all examples in a dataset to the same length.\n",
        "\n",
        "Moving on, here, we adopt a more sophisticated approach by developing a custom collate function that we can pass to the data loader.\n",
        "\n",
        "This custom collate function pads the training examples in each batch to have the same length, while allowing different batches to have different lengths.\n",
        "\n",
        "This approach minimizes unnecessary padding by only extending sequences to match the longest one in each batch, not the whole dataset.\n",
        "\n",
        "We can implement the padding process with a custom collate function as follows:\n",
        "\n",
        "Step 1: Find the longest sequence in the batch\n",
        "\n",
        "Step 2: Pad and prepare inputs\n",
        "\n",
        "Step 3: Remove extra padded token added earlier\n",
        "\n",
        "Step 4: Convert list of inputs to tensor and transfer to target device"
      ],
      "metadata": {
        "id": "e1_fpjigD_8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Finding the longest sequence in the batch\n",
        "    # and increasing the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Adding an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Padding sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Converting list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:40.037437Z",
          "iopub.execute_input": "2026-02-12T15:13:40.038224Z",
          "iopub.status.idle": "2026-02-12T15:13:40.043557Z",
          "shell.execute_reply.started": "2026-02-12T15:13:40.038193Z",
          "shell.execute_reply": "2026-02-12T15:13:40.042768Z"
        },
        "id": "xu0LP3cHD_8h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The custom_collate_draft_1 we implemented is designed to be integrated into a PyTorch DataLoader, but it can also function as a standalone tool.\n",
        "\n",
        "Here, we use it independently to test and verify that it operates as intended.\n",
        "\n",
        "Now, trying it on three different inputs that we\n",
        "want to assemble into a batch, where each example gets padded to the same length:"
      ],
      "metadata": {
        "id": "zBbKJxOyD_8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft_1(batch))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:44.131712Z",
          "iopub.execute_input": "2026-02-12T15:13:44.132497Z",
          "iopub.status.idle": "2026-02-12T15:13:44.176868Z",
          "shell.execute_reply.started": "2026-02-12T15:13:44.132464Z",
          "shell.execute_reply": "2026-02-12T15:13:44.176134Z"
        },
        "id": "FrzzwdQYD_8i",
        "outputId": "13156dd3-054c-49fd-8fbf-9a9543931bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see based on the preceding output, all inputs have been padded to the length of the longest input list, inputs_1 containing 5 token IDs.\n",
        "\n",
        "So far, we have just implemented our first custom collate function to create batches from lists of inputs.\n",
        "\n",
        "However, as you learned in previous lessons, we also need to create batches with the target token IDs, corresponding to the batch of input IDs.\n",
        "\n",
        "These target IDs are crucial because they represent what we want the model to generate and what we need during training to calculate the loss for the weight updates."
      ],
      "metadata": {
        "id": "uj2cRXxyD_8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATING TARGET TOKEN IDS FOR TRAINING:**\n",
        "\n",
        "Similar to pretraining an LLM, the target token IDs match the input token IDs but are shifted one position to the right.\n",
        "\n",
        "This setup allows the LLM to learn how to predict the next token in a sequence.\n",
        "\n",
        "The following updated collate function generates the target token IDs from the input token IDs:"
      ],
      "metadata": {
        "id": "jo9ibMqzD_8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Adding an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Padding sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncating the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shifting +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Converting list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:50.421855Z",
          "iopub.execute_input": "2026-02-12T15:13:50.422558Z",
          "iopub.status.idle": "2026-02-12T15:13:50.428588Z",
          "shell.execute_reply.started": "2026-02-12T15:13:50.42253Z",
          "shell.execute_reply": "2026-02-12T15:13:50.427731Z"
        },
        "id": "Bz1g8TouD_8k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Truncate the last token for inputs\n",
        "\n",
        "Step 2: Shift +1 to the right for targets"
      ],
      "metadata": {
        "id": "0hxU5rXTD_8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:54.002274Z",
          "iopub.execute_input": "2026-02-12T15:13:54.002549Z",
          "iopub.status.idle": "2026-02-12T15:13:54.009728Z",
          "shell.execute_reply.started": "2026-02-12T15:13:54.002524Z",
          "shell.execute_reply": "2026-02-12T15:13:54.008919Z"
        },
        "id": "vVKTk7z_D_8m",
        "outputId": "046b7b80-e3da-47b4-c79e-3a6748750523"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256, 50256, 50256, 50256],\n        [    8,     9, 50256, 50256, 50256]])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 1st tensor represents inputs.\n",
        "\n",
        "The 2nd tensor represents the targets\n",
        "\n",
        "In the next step, we assign a -100 placeholder value to all padding tokens.\n",
        "\n",
        "This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.\n",
        "\n",
        "In classification fine-tuning, we did not have to worry about this since we only trained the model based on the last output token.)\n",
        "\n",
        "We retain one end-of-text token, ID 50256, in the target list.\n",
        "\n",
        "This allows the LLM to learn when to generate an end-of-text token in response to instructions, which we use as an indicator that the generated response is complete.\n",
        "\n",
        "We modify our custom collate function to replace tokens with ID 50256 with -100 in the target lists.\n",
        "\n",
        "Additionally, we introduce an allowed_max_length parameter to optionally limit the length of the samples.\n",
        "\n",
        "This adjustment will be useful if you plan to work with your own datasets that exceed the 1024- token context size supported by the GPT-2 model.\n",
        "\n",
        "The code for this updated collate function is as follows:"
      ],
      "metadata": {
        "id": "kBibnIvdD_8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Finding the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Padding and preparing inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Adding an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Padding sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # Replacing all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # Optionally truncating to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Converting list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:13:57.601678Z",
          "iopub.execute_input": "2026-02-12T15:13:57.602295Z",
          "iopub.status.idle": "2026-02-12T15:13:57.608894Z",
          "shell.execute_reply.started": "2026-02-12T15:13:57.602265Z",
          "shell.execute_reply": "2026-02-12T15:13:57.608178Z"
        },
        "id": "8ZFpFPWrD_8o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Replace all but the first padding tokens in targets by ignore_index\n",
        "\n",
        "Step 2: Optionally truncate to maximum sequence length"
      ],
      "metadata": {
        "id": "2T1BgJtHD_8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:01.136822Z",
          "iopub.execute_input": "2026-02-12T15:14:01.137556Z",
          "iopub.status.idle": "2026-02-12T15:14:01.154799Z",
          "shell.execute_reply.started": "2026-02-12T15:14:01.137524Z",
          "shell.execute_reply": "2026-02-12T15:14:01.154168Z"
        },
        "id": "8JOk9gJ0D_8q",
        "outputId": "ddedf415-e146-41ae-935e-97605842fb08"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[    0,     1,     2,     3,     4],\n        [    5,     6, 50256, 50256, 50256],\n        [    7,     8,     9, 50256, 50256]])\ntensor([[    1,     2,     3,     4, 50256],\n        [    6, 50256,  -100,  -100,  -100],\n        [    8,     9, 50256,  -100,  -100]])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modified collate function works as expected, altering the target list by inserting the token ID -100.\n",
        "\n",
        "What is the logic behind this adjustment? Let's explore the underlying purpose of this modification.\n",
        "\n",
        "Consider the following simple and self-contained example where each output logit can correspond to a potential token from the model's vocabulary.\n",
        "\n",
        "Here's how we might calculate the cross entropy loss  during training when the model predicts a sequence of tokens, similar to what we have done pretraining the model, or when finetuning the model for classification:"
      ],
      "metadata": {
        "id": "xHYUnGFXD_8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_1 = torch.tensor(\n",
        "    [[-1.0, 1.0],  # 1st training example\n",
        "     [-0.5, 1.5]]  # 2nd training example\n",
        ")\n",
        "targets_1 = torch.tensor([0, 1])\n",
        "\n",
        "\n",
        "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
        "print(loss_1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:05.616566Z",
          "iopub.execute_input": "2026-02-12T15:14:05.616901Z",
          "iopub.status.idle": "2026-02-12T15:14:05.651925Z",
          "shell.execute_reply.started": "2026-02-12T15:14:05.616875Z",
          "shell.execute_reply": "2026-02-12T15:14:05.651326Z"
        },
        "id": "qtTS7S7CD_8r",
        "outputId": "546b4cc0-fdca-4359-b294-7d89957a6104"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor(1.1269)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding an additional token ID will, as we would expect, affect the loss calculation."
      ],
      "metadata": {
        "id": "NXLGkmmdD_8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits_2 = torch.tensor(\n",
        "    [[-1.0, 1.0],\n",
        "     [-0.5, 1.5],\n",
        "     [-0.5, 1.5]]  # New 3rd training example\n",
        ")\n",
        "targets_2 = torch.tensor([0, 1, 1])\n",
        "\n",
        "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
        "print(loss_2)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:11.086872Z",
          "iopub.execute_input": "2026-02-12T15:14:11.087182Z",
          "iopub.status.idle": "2026-02-12T15:14:11.094861Z",
          "shell.execute_reply.started": "2026-02-12T15:14:11.087159Z",
          "shell.execute_reply": "2026-02-12T15:14:11.093997Z"
        },
        "id": "2qqrO4jID_8t",
        "outputId": "c13f4339-aaff-486c-9d1b-05ebdbe6c89d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor(0.7936)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see what happens if we replace the third target token ID with -100:"
      ],
      "metadata": {
        "id": "iTyBFuvyD_8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets_3 = torch.tensor([0, 1, -100])\n",
        "\n",
        "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
        "print(loss_3)\n",
        "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:15.117162Z",
          "iopub.execute_input": "2026-02-12T15:14:15.117826Z",
          "iopub.status.idle": "2026-02-12T15:14:15.124621Z",
          "shell.execute_reply.started": "2026-02-12T15:14:15.117787Z",
          "shell.execute_reply": "2026-02-12T15:14:15.123857Z"
        },
        "id": "gqpIDVd5D_8v",
        "outputId": "55048ece-cd36-4a64-9eb6-f86274536f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor(1.1269)\nloss_1 == loss_3: tensor(True)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this result, we can see that the resulting loss on these 3 training examples is identical to the loss we calculated from the 2 training examples earlier.\n",
        "\n",
        "In other words, the cross entropy loss function ignored the third entry in the targets_3 vector, the token ID corresponding to -100.\n",
        "\n",
        "So, what's so special about -100 that it's ignored by the cross entropy loss? The default setting of the cross entropy function in PyTorch is cross_entropy(..., ignore_index=-100).\n",
        "\n",
        "This means that it ignores targets labeled with -100.\n",
        "\n",
        "We will take advantage of this ignore_index to ignore the additional end-oftext (padding) tokens that we used to pad the training examples to have the same length in each batch.\n",
        "\n",
        "However, we want to keep one 50256 (end-of-text) token ID in the targets because it helps the LLM to learn to generate end-of-text tokens, which we can use as an indicator that a response is complete."
      ],
      "metadata": {
        "id": "GWl--95BD_8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MASKING TARGET TOKEN IDS:**\n",
        "\n",
        "In addition to masking out padding tokens, it is also common to mask out the target token IDs that correspond to the instruction\n",
        "\n",
        "By masking out the target token IDs that correspond to the instruction, the LLM cross entropy loss is only computed for the generated response target IDs.\n",
        "\n",
        "By masking out the instruction tokens, the model is trained to focus on generating accurate responses rather than additionally also memorizing instructions, which can help with reducing overfitting.\n"
      ],
      "metadata": {
        "id": "ctUXCVJcD_8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3: CREATING DATALOADERS FROM AN INSTRUCTION DATASET**"
      ],
      "metadata": {
        "id": "AIfYyILaD_9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The custom_collate_fn includes code to move the input and target tensors (for example, torch.stack(inputs_lst).to(device))"
      ],
      "metadata": {
        "id": "IKFIs3OkD_9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:22.758551Z",
          "iopub.execute_input": "2026-02-12T15:14:22.759163Z",
          "iopub.status.idle": "2026-02-12T15:14:23.012901Z",
          "shell.execute_reply.started": "2026-02-12T15:14:22.759134Z",
          "shell.execute_reply": "2026-02-12T15:14:23.012025Z"
        },
        "id": "wslTISRBD_9B",
        "outputId": "727691bc-4775-4e86-f3b4-6dafe05bc9ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Device: cuda\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reuse the chosen device setting in custom_collate_fn when we plug it into the PyTorch DataLoader class later in this section, we use the partial function from Python's functools standard library to create a new version of the function with the device argument pre-filled.\n",
        "\n",
        "Additionally, we set the allowed_max_length to 1024, which truncates the data to the maximum context length supported by the GPT-2 model."
      ],
      "metadata": {
        "id": "_a_ioHCbD_9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:26.046854Z",
          "iopub.execute_input": "2026-02-12T15:14:26.047168Z",
          "iopub.status.idle": "2026-02-12T15:14:26.051164Z",
          "shell.execute_reply.started": "2026-02-12T15:14:26.047142Z",
          "shell.execute_reply": "2026-02-12T15:14:26.050505Z"
        },
        "id": "LGjSiDyMD_9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:28.851868Z",
          "iopub.execute_input": "2026-02-12T15:14:28.852165Z",
          "iopub.status.idle": "2026-02-12T15:14:28.908929Z",
          "shell.execute_reply.started": "2026-02-12T15:14:28.852143Z",
          "shell.execute_reply": "2026-02-12T15:14:28.908233Z"
        },
        "id": "RUyDW1G5D_9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(inputs.shape, targets.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:31.882039Z",
          "iopub.execute_input": "2026-02-12T15:14:31.882314Z",
          "iopub.status.idle": "2026-02-12T15:14:32.271984Z",
          "shell.execute_reply.started": "2026-02-12T15:14:31.882291Z",
          "shell.execute_reply": "2026-02-12T15:14:32.271255Z"
        },
        "id": "JdxYyoOuD_9C",
        "outputId": "cdedc8be-c630-4cc1-fdb7-4b52a557d8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Train loader:\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 76]) torch.Size([8, 76])\ntorch.Size([8, 73]) torch.Size([8, 73])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 72]) torch.Size([8, 72])\ntorch.Size([8, 80]) torch.Size([8, 80])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 62]) torch.Size([8, 62])\ntorch.Size([8, 75]) torch.Size([8, 75])\ntorch.Size([8, 62]) torch.Size([8, 62])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 77]) torch.Size([8, 77])\ntorch.Size([8, 69]) torch.Size([8, 69])\ntorch.Size([8, 79]) torch.Size([8, 79])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 80]) torch.Size([8, 80])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 69]) torch.Size([8, 69])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 60]) torch.Size([8, 60])\ntorch.Size([8, 59]) torch.Size([8, 59])\ntorch.Size([8, 69]) torch.Size([8, 69])\ntorch.Size([8, 63]) torch.Size([8, 63])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 76]) torch.Size([8, 76])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 91]) torch.Size([8, 91])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 75]) torch.Size([8, 75])\ntorch.Size([8, 89]) torch.Size([8, 89])\ntorch.Size([8, 59]) torch.Size([8, 59])\ntorch.Size([8, 88]) torch.Size([8, 88])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 70]) torch.Size([8, 70])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 74]) torch.Size([8, 74])\ntorch.Size([8, 76]) torch.Size([8, 76])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 75]) torch.Size([8, 75])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 69]) torch.Size([8, 69])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 60]) torch.Size([8, 60])\ntorch.Size([8, 60]) torch.Size([8, 60])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 80]) torch.Size([8, 80])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 58]) torch.Size([8, 58])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 63]) torch.Size([8, 63])\ntorch.Size([8, 87]) torch.Size([8, 87])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 71]) torch.Size([8, 71])\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 65]) torch.Size([8, 65])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 60]) torch.Size([8, 60])\ntorch.Size([8, 72]) torch.Size([8, 72])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 70]) torch.Size([8, 70])\ntorch.Size([8, 57]) torch.Size([8, 57])\ntorch.Size([8, 72]) torch.Size([8, 72])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 62]) torch.Size([8, 62])\ntorch.Size([8, 74]) torch.Size([8, 74])\ntorch.Size([8, 80]) torch.Size([8, 80])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 70]) torch.Size([8, 70])\ntorch.Size([8, 91]) torch.Size([8, 91])\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 80]) torch.Size([8, 80])\ntorch.Size([8, 81]) torch.Size([8, 81])\ntorch.Size([8, 74]) torch.Size([8, 74])\ntorch.Size([8, 82]) torch.Size([8, 82])\ntorch.Size([8, 63]) torch.Size([8, 63])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 68]) torch.Size([8, 68])\ntorch.Size([8, 67]) torch.Size([8, 67])\ntorch.Size([8, 77]) torch.Size([8, 77])\ntorch.Size([8, 91]) torch.Size([8, 91])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 61]) torch.Size([8, 61])\ntorch.Size([8, 75]) torch.Size([8, 75])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 78]) torch.Size([8, 78])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 64]) torch.Size([8, 64])\ntorch.Size([8, 83]) torch.Size([8, 83])\ntorch.Size([8, 66]) torch.Size([8, 66])\ntorch.Size([8, 74]) torch.Size([8, 74])\ntorch.Size([8, 69]) torch.Size([8, 69])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the preceding output, we can see that the first input and target batch have dimensions 8Ã—61, where 8 represents the batch size, and 61 is the number of tokens in each training example in this batch.\n",
        "\n",
        "The second input and target batch have a different number of tokens, for instance, 76.\n",
        "\n",
        "As we saw in the preceding code output, thanks to our custom collate function, the data loader is able to create batches of different lengths."
      ],
      "metadata": {
        "id": "PFds1MQAD_9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 4: LOADING A PRETRAINED LLM**\n",
        "\n",
        "Before beginning instruction finetuning, we first load a pretrained GPT model,\n",
        "\n",
        "Instead of using the smallest 124 million parameter model as before, we load the medium-sized model with 355 million parameters.\n",
        "\n",
        "The reason for this choice is that the 124 million parameter model is too limited in capacity to achieve qualitatively satisfactory results via instruction finetuning.\n",
        "\n",
        "This is done using the same code as in section 5.5 of chapter 5 and section 6.4 of the previous chapter, except that we now specify \"gpt2-medium (355M)\" instead of \"gpt2-small (124M)\".\n",
        "\n",
        "Please note that executing the code provided below will initiate the download of the medium-sized GPT model, which has a storage requirement of approximately 1.42 gigabytes."
      ],
      "metadata": {
        "id": "P3mRI-OgD_9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!find /kaggle/input -name \"gpt_download3.py\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:48.896463Z",
          "iopub.execute_input": "2026-02-12T15:14:48.896819Z",
          "iopub.status.idle": "2026-02-12T15:14:49.034518Z",
          "shell.execute_reply.started": "2026-02-12T15:14:48.896791Z",
          "shell.execute_reply": "2026-02-12T15:14:49.033862Z"
        },
        "id": "JBf60kvSD_9E",
        "outputId": "1278a74a-c338-460c-8c19-887571732941"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/models/sc1411/gpt-download3/pytorch/default/1/gpt_download3.py\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /kaggle/input/models/sc1411/gpt-download3/pytorch/default/1/gpt_download3.py /kaggle/working/"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:52.331957Z",
          "iopub.execute_input": "2026-02-12T15:14:52.332558Z",
          "iopub.status.idle": "2026-02-12T15:14:52.469218Z",
          "shell.execute_reply.started": "2026-02-12T15:14:52.332521Z",
          "shell.execute_reply": "2026-02-12T15:14:52.468188Z"
        },
        "id": "QR6EwpmiD_9F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:14:55.846693Z",
          "iopub.execute_input": "2026-02-12T15:14:55.847479Z",
          "iopub.status.idle": "2026-02-12T15:15:10.411494Z",
          "shell.execute_reply.started": "2026-02-12T15:14:55.847445Z",
          "shell.execute_reply": "2026-02-12T15:15:10.410899Z"
        },
        "id": "CqhIWa5_D_9F",
        "outputId": "de3407f9-af0a-4db4-8218-8d7ad724966e"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2026-02-12 15:14:57.400456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770909297.579253      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770909297.630960      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770909298.065304      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770909298.065341      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770909298.065344      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770909298.065347      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:49:49.931808Z",
          "iopub.execute_input": "2026-02-12T15:49:49.932378Z",
          "iopub.status.idle": "2026-02-12T15:49:49.936912Z",
          "shell.execute_reply.started": "2026-02-12T15:49:49.932348Z",
          "shell.execute_reply": "2026-02-12T15:49:49.936074Z"
        },
        "id": "CQREr4odD_9H"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:49:54.592261Z",
          "iopub.execute_input": "2026-02-12T15:49:54.592594Z",
          "iopub.status.idle": "2026-02-12T15:49:54.606483Z",
          "shell.execute_reply.started": "2026-02-12T15:49:54.592566Z",
          "shell.execute_reply": "2026-02-12T15:49:54.605761Z"
        },
        "id": "3nr4_TZGD_9I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:18:44.292026Z",
          "iopub.execute_input": "2026-02-12T15:18:44.292391Z",
          "iopub.status.idle": "2026-02-12T15:18:46.781919Z",
          "shell.execute_reply.started": "2026-02-12T15:18:44.29236Z",
          "shell.execute_reply": "2026-02-12T15:18:46.781188Z"
        },
        "id": "HmlDekTKD_9K",
        "outputId": "a65051b0-4869-4d04-a811-9a8af26eb0d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "File already exists and is up-to-date: gpt2/355M/checkpoint\nFile already exists and is up-to-date: gpt2/355M/encoder.json\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "File already exists and is up-to-date: gpt2/355M/hparams.json\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\nFile already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:45:03.762481Z",
          "iopub.execute_input": "2026-02-12T15:45:03.762823Z",
          "iopub.status.idle": "2026-02-12T15:45:03.768662Z",
          "shell.execute_reply.started": "2026-02-12T15:45:03.762796Z",
          "shell.execute_reply": "2026-02-12T15:45:03.767802Z"
        },
        "id": "9JtR_JonD_9M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:45:47.901929Z",
          "iopub.execute_input": "2026-02-12T15:45:47.902524Z",
          "iopub.status.idle": "2026-02-12T15:45:47.908046Z",
          "shell.execute_reply.started": "2026-02-12T15:45:47.902495Z",
          "shell.execute_reply": "2026-02-12T15:45:47.907199Z"
        },
        "id": "1Gi_sjDyD_9O"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:46:37.986559Z",
          "iopub.execute_input": "2026-02-12T15:46:37.986979Z",
          "iopub.status.idle": "2026-02-12T15:46:37.991811Z",
          "shell.execute_reply.started": "2026-02-12T15:46:37.986949Z",
          "shell.execute_reply": "2026-02-12T15:46:37.991116Z"
        },
        "id": "SJn8qcytD_9P"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:47:02.387272Z",
          "iopub.execute_input": "2026-02-12T15:47:02.388037Z",
          "iopub.status.idle": "2026-02-12T15:47:02.397565Z",
          "shell.execute_reply.started": "2026-02-12T15:47:02.388003Z",
          "shell.execute_reply": "2026-02-12T15:47:02.396952Z"
        },
        "id": "gb6RfQLvD_9Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:47:17.891573Z",
          "iopub.execute_input": "2026-02-12T15:47:17.891906Z",
          "iopub.status.idle": "2026-02-12T15:47:17.898308Z",
          "shell.execute_reply.started": "2026-02-12T15:47:17.891878Z",
          "shell.execute_reply": "2026-02-12T15:47:17.897606Z"
        },
        "id": "1DVQebAWD_9Q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:47:32.817208Z",
          "iopub.execute_input": "2026-02-12T15:47:32.818013Z",
          "iopub.status.idle": "2026-02-12T15:47:32.824325Z",
          "shell.execute_reply.started": "2026-02-12T15:47:32.817963Z",
          "shell.execute_reply": "2026-02-12T15:47:32.823593Z"
        },
        "id": "EQvVjeJ0D_9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "IMFWrzMbD_9R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into finetuning the model in the next section, let's take a moment to assess the pretrained LLM's performance on one of the validation tasks by comparing its output to the expected response.\n",
        "\n",
        "This will give us a baseline understanding of how well the model performs on an instruction-following task right out of the box, prior to finetuning, and will help us appreciate the impact of finetuning later on."
      ],
      "metadata": {
        "id": "1A0RZu3cD_9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:50:21.146561Z",
          "iopub.execute_input": "2026-02-12T15:50:21.146979Z",
          "iopub.status.idle": "2026-02-12T15:50:21.165049Z",
          "shell.execute_reply.started": "2026-02-12T15:50:21.146949Z",
          "shell.execute_reply": "2026-02-12T15:50:21.164367Z"
        },
        "id": "Mzv_QaZsD_9S",
        "outputId": "fdec214b-62de-46c9-99d0-c43d9c6bbc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nConvert the active sentence to passive: 'The chef cooks the meal every day.'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Croping current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Getting the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focusing only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Applying softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Getting the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Appending sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T15:58:18.322629Z",
          "iopub.execute_input": "2026-02-12T15:58:18.323211Z",
          "iopub.status.idle": "2026-02-12T15:58:18.328665Z",
          "shell.execute_reply.started": "2026-02-12T15:58:18.323182Z",
          "shell.execute_reply": "2026-02-12T15:58:18.32798Z"
        },
        "id": "QU2Q5QbvD_9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:00:25.207428Z",
          "iopub.execute_input": "2026-02-12T16:00:25.208051Z",
          "iopub.status.idle": "2026-02-12T16:00:25.213221Z",
          "shell.execute_reply.started": "2026-02-12T16:00:25.208018Z",
          "shell.execute_reply": "2026-02-12T16:00:25.212383Z"
        },
        "id": "VjOEutE4D_9V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # adding batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # removing batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:00:29.066867Z",
          "iopub.execute_input": "2026-02-12T16:00:29.067628Z",
          "iopub.status.idle": "2026-02-12T16:00:31.295874Z",
          "shell.execute_reply.started": "2026-02-12T16:00:29.067597Z",
          "shell.execute_reply": "2026-02-12T16:00:31.29513Z"
        },
        "id": "f2uLTeLhD_9W",
        "outputId": "67e88cc0-2ecf-4cb9-e477-d8230644815d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Output text:\n Every effort moves you forward, but you must be careful. You must\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stopping generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Appending sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:03:22.797204Z",
          "iopub.execute_input": "2026-02-12T16:03:22.797804Z",
          "iopub.status.idle": "2026-02-12T16:03:22.804283Z",
          "shell.execute_reply.started": "2026-02-12T16:03:22.797778Z",
          "shell.execute_reply": "2026-02-12T16:03:22.803488Z"
        },
        "id": "hyg8DnSmD_9W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "C512i8cWD_9X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:09:12.412036Z",
          "iopub.execute_input": "2026-02-12T16:09:12.412613Z",
          "iopub.status.idle": "2026-02-12T16:09:16.022254Z",
          "shell.execute_reply.started": "2026-02-12T16:09:12.412586Z",
          "shell.execute_reply": "2026-02-12T16:09:16.021401Z"
        },
        "id": "zUWNzsJxD_9Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:09:22.839398Z",
          "iopub.execute_input": "2026-02-12T16:09:22.839675Z",
          "iopub.status.idle": "2026-02-12T16:09:37.58994Z",
          "shell.execute_reply.started": "2026-02-12T16:09:22.839652Z",
          "shell.execute_reply": "2026-02-12T16:09:37.589141Z"
        },
        "id": "K6zJK9iYD_9Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate function returns the combined input and output text.\n",
        "\n",
        "This behavior was convenient in previous chapters since pretrained LLMs are primarily designed as text-completion models, where the input and output are concatenated to create a coherent and legible text.\n",
        "\n",
        "However, when evaluating the model's performance on a specific task, we often want to focus solely on the model's generated response.\n",
        "\n",
        "To isolate the model's response text, we need to subtract the length of the input instruction from the start of the generated_text:"
      ],
      "metadata": {
        "id": "kM2mobZ5D_9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:09:43.50835Z",
          "iopub.execute_input": "2026-02-12T16:09:43.509135Z",
          "iopub.status.idle": "2026-02-12T16:09:43.513403Z",
          "shell.execute_reply.started": "2026-02-12T16:09:43.509102Z",
          "shell.execute_reply": "2026-02-12T16:09:43.51279Z"
        },
        "id": "i5AKGLZuD_9a",
        "outputId": "e542f923-9d1a-40f0-ba6a-7598c4b04e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "### Response:\n\nThe chef cooks the meal every day.\n\n### Instruction:\n\nConvert the active sentence to passive: 'The chef cooks the\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet removes the input text from the beginning of the generated_text, leaving us with only the model's generated response. The strip() function is then applied to remove any leading or trailing whitespace characters. The output is as follows:\n",
        "\n",
        "As we can see from the output, the pretrained model is not yet capable of correctly following the given instruction.\n",
        "\n",
        "While it does create a \"Response\" section, it simply repeats the original input sentence and part of the instruction, failing to convert the active sentence to passive voice as requested."
      ],
      "metadata": {
        "id": "N3SmefLpD_9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 5: FINETUNING THE LLM ON INSTRUCTION DATA**\n",
        "\n"
      ],
      "metadata": {
        "id": "7nd1CYmrD_9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:23:24.431615Z",
          "iopub.execute_input": "2026-02-12T16:23:24.432328Z",
          "iopub.status.idle": "2026-02-12T16:23:24.442341Z",
          "shell.execute_reply.started": "2026-02-12T16:23:24.432299Z",
          "shell.execute_reply": "2026-02-12T16:23:24.441686Z"
        },
        "id": "og1uHKxdD_9c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:23:30.59114Z",
          "iopub.execute_input": "2026-02-12T16:23:30.591681Z",
          "iopub.status.idle": "2026-02-12T16:23:32.332317Z",
          "shell.execute_reply.started": "2026-02-12T16:23:30.591652Z",
          "shell.execute_reply": "2026-02-12T16:23:32.331468Z"
        },
        "id": "3x1As9t7D_9g",
        "outputId": "f165c0f6-be3c-4021-c7f9-bd935d123fc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training loss: 3.825909376144409\nValidation loss: 3.7619347095489504\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the model and data loaders prepared, we can now proceed to train the model.\n",
        "\n",
        "The following code sets up the training process, including initializing the optimizer, setting the number of epochs, and defining the evaluation frequency and starting context to evaluate generated LLM responses during training based on the first validation set instruction (val_data[0]):"
      ],
      "metadata": {
        "id": "ts6i-HM6D_9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:27:22.372212Z",
          "iopub.execute_input": "2026-02-12T16:27:22.372517Z",
          "iopub.status.idle": "2026-02-12T16:27:22.377135Z",
          "shell.execute_reply.started": "2026-02-12T16:27:22.372489Z",
          "shell.execute_reply": "2026-02-12T16:27:22.376399Z"
        },
        "id": "AC7SZ2WrD_9h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:31:00.243994Z",
          "iopub.execute_input": "2026-02-12T16:31:00.244312Z",
          "iopub.status.idle": "2026-02-12T16:31:00.249603Z",
          "shell.execute_reply.started": "2026-02-12T16:31:00.244284Z",
          "shell.execute_reply": "2026-02-12T16:31:00.248832Z"
        },
        "id": "igzbBPQND_9i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:43:59.850487Z",
          "iopub.execute_input": "2026-02-12T16:43:59.850852Z",
          "iopub.status.idle": "2026-02-12T16:44:00.530513Z",
          "shell.execute_reply.started": "2026-02-12T16:43:59.850823Z",
          "shell.execute_reply": "2026-02-12T16:44:00.529907Z"
        },
        "id": "ycEo8p8xD_9i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:44:11.630328Z",
          "iopub.execute_input": "2026-02-12T16:44:11.630638Z",
          "iopub.status.idle": "2026-02-12T16:44:11.634639Z",
          "shell.execute_reply.started": "2026-02-12T16:44:11.630613Z",
          "shell.execute_reply": "2026-02-12T16:44:11.633741Z"
        },
        "id": "0yEwhflBD_9j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:44:15.97592Z",
          "iopub.execute_input": "2026-02-12T16:44:15.976207Z",
          "iopub.status.idle": "2026-02-12T16:52:29.961492Z",
          "shell.execute_reply.started": "2026-02-12T16:44:15.976182Z",
          "shell.execute_reply": "2026-02-12T16:52:29.960794Z"
        },
        "id": "uJ1YVGyjD_9j",
        "outputId": "7d261902-e9fc-45eb-8f87-33766e302341"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Ep 1 (Step 000000): Train loss 0.492, Val loss 0.725\nEp 1 (Step 000005): Train loss 0.587, Val loss 0.704\nEp 1 (Step 000010): Train loss 0.485, Val loss 0.703\nEp 1 (Step 000015): Train loss 0.527, Val loss 0.708\nEp 1 (Step 000020): Train loss 0.459, Val loss 0.716\nEp 1 (Step 000025): Train loss 0.496, Val loss 0.711\nEp 1 (Step 000030): Train loss 0.490, Val loss 0.708\nEp 1 (Step 000035): Train loss 0.483, Val loss 0.704\nEp 1 (Step 000040): Train loss 0.428, Val loss 0.710\nEp 1 (Step 000045): Train loss 0.425, Val loss 0.704\nEp 1 (Step 000050): Train loss 0.428, Val loss 0.691\nEp 1 (Step 000055): Train loss 0.524, Val loss 0.681\nEp 1 (Step 000060): Train loss 0.455, Val loss 0.680\nEp 1 (Step 000065): Train loss 0.418, Val loss 0.673\nEp 1 (Step 000070): Train loss 0.381, Val loss 0.663\nEp 1 (Step 000075): Train loss 0.391, Val loss 0.670\nEp 1 (Step 000080): Train loss 0.401, Val loss 0.677\nEp 1 (Step 000085): Train loss 0.332, Val loss 0.678\nEp 1 (Step 000090): Train loss 0.395, Val loss 0.664\nEp 1 (Step 000095): Train loss 0.341, Val loss 0.672\nEp 1 (Step 000100): Train loss 0.369, Val loss 0.666\nEp 1 (Step 000105): Train loss 0.373, Val loss 0.652\nEp 1 (Step 000110): Train loss 0.382, Val loss 0.651\nEp 1 (Step 000115): Train loss 0.356, Val loss 0.648\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: The following is an instruction that describes a task.\nEp 2 (Step 000120): Train loss 0.311, Val loss 0.658\nEp 2 (Step 000125): Train loss 0.311, Val loss 0.683\nEp 2 (Step 000130): Train loss 0.312, Val loss 0.700\nEp 2 (Step 000135): Train loss 0.297, Val loss 0.716\nEp 2 (Step 000140): Train loss 0.281, Val loss 0.713\nEp 2 (Step 000145): Train loss 0.274, Val loss 0.706\nEp 2 (Step 000150): Train loss 0.280, Val loss 0.696\nEp 2 (Step 000155): Train loss 0.322, Val loss 0.692\nEp 2 (Step 000160): Train loss 0.301, Val loss 0.695\nEp 2 (Step 000165): Train loss 0.276, Val loss 0.692\nEp 2 (Step 000170): Train loss 0.246, Val loss 0.694\nEp 2 (Step 000175): Train loss 0.250, Val loss 0.686\nEp 2 (Step 000180): Train loss 0.279, Val loss 0.674\nEp 2 (Step 000185): Train loss 0.320, Val loss 0.673\nEp 2 (Step 000190): Train loss 0.258, Val loss 0.665\nEp 2 (Step 000195): Train loss 0.250, Val loss 0.653\nEp 2 (Step 000200): Train loss 0.247, Val loss 0.653\nEp 2 (Step 000205): Train loss 0.267, Val loss 0.646\nEp 2 (Step 000210): Train loss 0.265, Val loss 0.647\nEp 2 (Step 000215): Train loss 0.301, Val loss 0.657\nEp 2 (Step 000220): Train loss 0.225, Val loss 0.676\nEp 2 (Step 000225): Train loss 0.247, Val loss 0.700\nEp 2 (Step 000230): Train loss 0.231, Val loss 0.690\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: The chef cooks the meal every day.\nEp 3 (Step 000235): Train loss 0.270, Val loss 0.687\nEp 3 (Step 000240): Train loss 0.225, Val loss 0.701\nEp 3 (Step 000245): Train loss 0.221, Val loss 0.713\nEp 3 (Step 000250): Train loss 0.191, Val loss 0.708\nEp 3 (Step 000255): Train loss 0.232, Val loss 0.696\nEp 3 (Step 000260): Train loss 0.221, Val loss 0.696\nEp 3 (Step 000265): Train loss 0.227, Val loss 0.710\nEp 3 (Step 000270): Train loss 0.236, Val loss 0.722\nEp 3 (Step 000275): Train loss 0.222, Val loss 0.712\nEp 3 (Step 000280): Train loss 0.228, Val loss 0.715\nEp 3 (Step 000285): Train loss 0.234, Val loss 0.724\nEp 3 (Step 000290): Train loss 0.213, Val loss 0.731\nEp 3 (Step 000295): Train loss 0.207, Val loss 0.735\nEp 3 (Step 000300): Train loss 0.205, Val loss 0.731\nEp 3 (Step 000305): Train loss 0.222, Val loss 0.728\nEp 3 (Step 000310): Train loss 0.218, Val loss 0.720\nEp 3 (Step 000315): Train loss 0.213, Val loss 0.721\nEp 3 (Step 000320): Train loss 0.218, Val loss 0.736\nEp 3 (Step 000325): Train loss 0.196, Val loss 0.723\nEp 3 (Step 000330): Train loss 0.192, Val loss 0.715\nEp 3 (Step 000335): Train loss 0.193, Val loss 0.713\nEp 3 (Step 000340): Train loss 0.209, Val loss 0.708\nEp 3 (Step 000345): Train loss 0.204, Val loss 0.707\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared by the chef every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\nEp 4 (Step 000350): Train loss 0.187, Val loss 0.711\nEp 4 (Step 000355): Train loss 0.191, Val loss 0.746\nEp 4 (Step 000360): Train loss 0.205, Val loss 0.787\nEp 4 (Step 000365): Train loss 0.195, Val loss 0.782\nEp 4 (Step 000370): Train loss 0.207, Val loss 0.768\nEp 4 (Step 000375): Train loss 0.195, Val loss 0.755\nEp 4 (Step 000380): Train loss 0.178, Val loss 0.747\nEp 4 (Step 000385): Train loss 0.199, Val loss 0.757\nEp 4 (Step 000390): Train loss 0.199, Val loss 0.766\nEp 4 (Step 000395): Train loss 0.191, Val loss 0.766\nEp 4 (Step 000400): Train loss 0.188, Val loss 0.755\nEp 4 (Step 000405): Train loss 0.189, Val loss 0.735\nEp 4 (Step 000410): Train loss 0.187, Val loss 0.729\nEp 4 (Step 000415): Train loss 0.188, Val loss 0.725\nEp 4 (Step 000420): Train loss 0.169, Val loss 0.725\nEp 4 (Step 000425): Train loss 0.177, Val loss 0.734\nEp 4 (Step 000430): Train loss 0.181, Val loss 0.748\nEp 4 (Step 000435): Train loss 0.180, Val loss 0.752\nEp 4 (Step 000440): Train loss 0.168, Val loss 0.761\nEp 4 (Step 000445): Train loss 0.173, Val loss 0.758\nEp 4 (Step 000450): Train loss 0.167, Val loss 0.743\nEp 4 (Step 000455): Train loss 0.176, Val loss 0.755\nEp 4 (Step 000460): Train loss 0.170, Val loss 0.766\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Calculate the area of a triangle\nEp 5 (Step 000465): Train loss 0.182, Val loss 0.771\nEp 5 (Step 000470): Train loss 0.177, Val loss 0.791\nEp 5 (Step 000475): Train loss 0.174, Val loss 0.808\nEp 5 (Step 000480): Train loss 0.168, Val loss 0.819\nEp 5 (Step 000485): Train loss 0.167, Val loss 0.809\nEp 5 (Step 000490): Train loss 0.183, Val loss 0.779\nEp 5 (Step 000495): Train loss 0.191, Val loss 0.756\nEp 5 (Step 000500): Train loss 0.168, Val loss 0.749\nEp 5 (Step 000505): Train loss 0.186, Val loss 0.741\nEp 5 (Step 000510): Train loss 0.174, Val loss 0.748\nEp 5 (Step 000515): Train loss 0.155, Val loss 0.759\nEp 5 (Step 000520): Train loss 0.167, Val loss 0.778\nEp 5 (Step 000525): Train loss 0.175, Val loss 0.778\nEp 5 (Step 000530): Train loss 0.173, Val loss 0.763\nEp 5 (Step 000535): Train loss 0.154, Val loss 0.762\nEp 5 (Step 000540): Train loss 0.175, Val loss 0.764\nEp 5 (Step 000545): Train loss 0.164, Val loss 0.751\nEp 5 (Step 000550): Train loss 0.169, Val loss 0.744\nEp 5 (Step 000555): Train loss 0.171, Val loss 0.755\nEp 5 (Step 000560): Train loss 0.179, Val loss 0.783\nEp 5 (Step 000565): Train loss 0.189, Val loss 0.786\nEp 5 (Step 000570): Train loss 0.168, Val loss 0.783\nEp 5 (Step 000575): Train loss 0.182, Val loss 0.791\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom? \nTraining completed in 8.23 minutes.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see based on the outputs above, the model trains well, as we can tell based on the decreasing training loss and validation loss values.\n",
        "\n",
        "Furthermore, based on the response text printed after each epoch, we can see that the model almost correctly follows the instruction to convert the input sentence 'The chef cooks the meal every day.' into passive voice 'The meal is prepared every day by the chef.' (We will properly format and evaluate the responses in a later section.\n",
        "\n",
        "To get better results, we need to finetune the model for more epochs."
      ],
      "metadata": {
        "id": "MYNOaf6eD_9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:56:34.431499Z",
          "iopub.execute_input": "2026-02-12T16:56:34.432327Z",
          "iopub.status.idle": "2026-02-12T16:56:34.437924Z",
          "shell.execute_reply.started": "2026-02-12T16:56:34.432296Z",
          "shell.execute_reply": "2026-02-12T16:56:34.437274Z"
        },
        "id": "yMF63HYjD_9k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T16:56:39.510867Z",
          "iopub.execute_input": "2026-02-12T16:56:39.511466Z",
          "iopub.status.idle": "2026-02-12T16:56:40.532759Z",
          "shell.execute_reply.started": "2026-02-12T16:56:39.511436Z",
          "shell.execute_reply": "2026-02-12T16:56:40.531947Z"
        },
        "id": "uBrGMH7nD_9l",
        "outputId": "93128851-005e-4246-c7f0-c26d8a835ac4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 500x300 with 2 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in the loss plot shown above, the model's performance on both the training and validation sets improves substantially over the course of training.\n",
        "\n",
        "The rapid decrease in losses during the initial phase indicates that the model is quickly learning meaningful patterns and representations from the data. Then, as training progresses to the second epoch, the losses continue to decrease but at a slower rate, suggesting that the model is finetuning its learned representations and converging to a stable solution.\n",
        "\n",
        "While the loss plot indicates that the model is training effectively, the most crucial aspect is its performance in terms of response quality and correctness. In the remaining sections of this chapter, we will extract the responses and store them in a format that allows us to evaluate and quantify the response quality."
      ],
      "metadata": {
        "id": "8vgetlHGD_9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6: EXTRACTING AND SAVING RESPONSES**\n",
        "\n",
        "After finetuning the LLM on the training portion of the instruction dataset as described in the previous section, we now proceed to evaluate its performance on the held-out test set.\n",
        "\n",
        "To accomplish this, we first extract the model-generated responses for each input in the test dataset and collect them for manual analysis\n",
        "\n",
        "Step 1: Iterate over the first 3 test set samples\n",
        "\n",
        "Step 2: Use the generate function defined earlier\n",
        "\n",
        "The generate function returns the combined input and output text, so we use slicing and the .replace() method on the generated_text contents to extract the model's response.\n",
        "\n",
        "The instructions, followed by the given test set response and model response are shown below:"
      ],
      "metadata": {
        "id": "QT66JqdID_9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T17:17:23.700204Z",
          "iopub.execute_input": "2026-02-12T17:17:23.700913Z",
          "iopub.status.idle": "2026-02-12T17:17:25.603575Z",
          "shell.execute_reply.started": "2026-02-12T17:17:23.70088Z",
          "shell.execute_reply": "2026-02-12T17:17:25.602922Z"
        },
        "id": "jgkMCj5vD_9o",
        "outputId": "11159455-11c3-4d85-826a-0f0120e4be8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nRewrite the sentence using a simile.\n\n### Input:\nThe car is very fast.\n\nCorrect response:\n>> The car is as fast as lightning.\n\nModel response:\n>> The car is as fast as a cheetah.\n-------------------------------------\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nWhat type of cloud is typically associated with thunderstorms?\n\nCorrect response:\n>> The type of cloud typically associated with thunderstorms is cumulonimbus.\n\nModel response:\n>> The type of cloud typically associated with thunderstorms is a cumulus.\n-------------------------------------\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\nName the author of 'Pride and Prejudice'.\n\nCorrect response:\n>> Jane Austen.\n\nModel response:\n>> The author of 'Pride and Prejudice' is Elizabeth Bennet.\n-------------------------------------\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, instruction-finetuned LLMs such as chatbots are evaluated via multiple approaches:\n",
        "\n",
        "1. Short-answer and multiple choice benchmarks such as MMLU (\"Measuring Massive Multitask Language Understanding,\" https://arxiv.org/abs/2009. 03300), which test the general knowledge of a model.\n",
        "\n",
        "2. Human preference comparison to other LLMs, such as LMSYS chatbot arena (https://arena.lmsys.org).\n",
        "\n",
        "3. Automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval (https://tatsulab.github.io/alpaca_eval/). completes the request.\n",
        "\n",
        "Considering the scale of the task at hand, we will implement an approach similar to method 3, which involves evaluating the responses automatically using another LLM.\n",
        "\n",
        "This will allow us to efficiently assess the quality of the generated responses without the need for extensive human involvement, thereby saving time and resources while still obtaining meaningful performance indicators.\n",
        "\n",
        "To prepare the responses for this evaluation process, we append the generated model responses to the test_set dictionary and save the updated data as an \"instructiondata-with-response.json\" file for record keeping."
      ],
      "metadata": {
        "id": "q7zxdjyoD_9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T17:25:29.846388Z",
          "iopub.execute_input": "2026-02-12T17:25:29.84673Z",
          "iopub.status.idle": "2026-02-12T17:26:49.508597Z",
          "shell.execute_reply.started": "2026-02-12T17:25:29.846701Z",
          "shell.execute_reply": "2026-02-12T17:26:49.507927Z"
        },
        "id": "ISMg93JcD_9p",
        "outputId": "ef63fc52-3208-4644-fe63-2e53389d70c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [01:19<00:00,  1.38it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T17:26:57.406603Z",
          "iopub.execute_input": "2026-02-12T17:26:57.407328Z",
          "iopub.status.idle": "2026-02-12T17:26:57.411626Z",
          "shell.execute_reply.started": "2026-02-12T17:26:57.407296Z",
          "shell.execute_reply": "2026-02-12T17:26:57.410836Z"
        },
        "id": "yfBfbl5RD_9q",
        "outputId": "9dd3f5c5-56a1-4c51-cdfe-1fee48aa27fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "\n",
        "# Load model via\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T17:27:03.036216Z",
          "iopub.execute_input": "2026-02-12T17:27:03.036552Z",
          "iopub.status.idle": "2026-02-12T17:27:05.124952Z",
          "shell.execute_reply.started": "2026-02-12T17:27:03.036525Z",
          "shell.execute_reply": "2026-02-12T17:27:05.124107Z"
        },
        "id": "pRPgAKlFD_9q",
        "outputId": "3b4882a1-1637-4003-9dd8-03a103c77c9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model saved as gpt2-medium355M-sft.pth\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 7: EVALUATING THE FINE-TUNED LLM**\n",
        "\n"
      ],
      "metadata": {
        "id": "Nwey7ewWD_9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Download and Install Ollama\n",
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama\n",
        "!cp ollama /usr/local/bin/\n",
        "\n",
        "# 2. Start the server in the background\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "with open(\"ollama.log\", \"w\") as f:\n",
        "    # Use the absolute path just to be extra safe\n",
        "    subprocess.Popen([\"/usr/local/bin/ollama\", \"serve\"], stdout=f, stderr=f)\n",
        "\n",
        "# 3. Wait for it to initialize\n",
        "time.sleep(5)\n",
        "print(\"Ollama is installed and starting...\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T18:10:56.3821Z",
          "iopub.execute_input": "2026-02-12T18:10:56.382567Z",
          "iopub.status.idle": "2026-02-12T18:10:57.186713Z",
          "shell.execute_reply.started": "2026-02-12T18:10:56.382531Z",
          "shell.execute_reply": "2026-02-12T18:10:57.185634Z"
        },
        "id": "5sKOQjpdD_9r",
        "outputId": "445a4c2a-2542-43cc-e9d7-a60c727af87b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100     9  100     9    0     0     20      0 --:--:-- --:--:-- --:--:--    20\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_55/3000023846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ollama.log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Use the absolute path just to be extra safe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"/usr/local/bin/ollama\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serve\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 3. Wait for it to initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 8] Exec format error: '/usr/local/bin/ollama'"
          ],
          "ename": "OSError",
          "evalue": "[Errno 8] Exec format error: '/usr/local/bin/ollama'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "def check_if_running(process_name):\n",
        "    running = False\n",
        "    for proc in psutil.process_iter([\"name\"]):\n",
        "        if process_name in proc.info[\"name\"]:\n",
        "            running = True\n",
        "            break\n",
        "    return running\n",
        "\n",
        "ollama_running = check_if_running(\"ollama\")\n",
        "\n",
        "if not ollama_running:\n",
        "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
        "print(\"Ollama running:\", check_if_running(\"ollama\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-12T18:07:44.580817Z",
          "iopub.execute_input": "2026-02-12T18:07:44.581292Z",
          "iopub.status.idle": "2026-02-12T18:07:44.593409Z",
          "shell.execute_reply.started": "2026-02-12T18:07:44.581257Z",
          "shell.execute_reply": "2026-02-12T18:07:44.592538Z"
        },
        "id": "JiIpG_UZD_9s",
        "outputId": "c38bf822-ae5e-4329-a6a1-86b88362db3c"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_55/242962959.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mollama_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ollama not running. Launch ollama before proceeding.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ollama running:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_if_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ollama\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Ollama not running. Launch ollama before proceeding."
          ],
          "ename": "RuntimeError",
          "evalue": "Ollama not running. Launch ollama before proceeding.",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "An alternative to the ollama run command for interacting with the model is through its REST API using Python.\n",
        "\n",
        "The following query_model function demonstrates how to use the API:\n",
        "\n",
        "Step 1: Create the data payload as a dictionary\n",
        "\n",
        "Step 2: Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "\n",
        "Step 3: Create a request object, setting the method to POST and adding necessary headers\n",
        "\n",
        "Step 4: Send the request and capture the response"
      ],
      "metadata": {
        "id": "A_sU1p6xD_9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "def query_model(\n",
        "    prompt,\n",
        "    model=\"llama3\",\n",
        "    url=\"http://localhost:11434/api/chat\"\n",
        "):\n",
        "    # Create the data payload as a dictionary\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"options\": {     # Settings below are required for deterministic responses\n",
        "            \"seed\": 123,\n",
        "            \"temperature\": 0,\n",
        "            \"num_ctx\": 2048\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    # Convert the dictionary to a JSON formatted string and encode it to bytes\n",
        "    payload = json.dumps(data).encode(\"utf-8\")\n",
        "\n",
        "    # Create a request object, setting the method to POST and adding necessary headers\n",
        "    request = urllib.request.Request(\n",
        "        url,\n",
        "        data=payload,\n",
        "        method=\"POST\"\n",
        "    )\n",
        "    request.add_header(\"Content-Type\", \"application/json\")\n",
        "\n",
        "    # Send the request and capture the response\n",
        "    response_data = \"\"\n",
        "    with urllib.request.urlopen(request) as response:\n",
        "        # Read and decode the response\n",
        "        while True:\n",
        "            line = response.readline().decode(\"utf-8\")\n",
        "            if not line:\n",
        "                break\n",
        "            response_json = json.loads(line)\n",
        "            response_data += response_json[\"message\"][\"content\"]\n",
        "\n",
        "    return response_data"
      ],
      "metadata": {
        "trusted": true,
        "id": "2wd8aSWAD_9s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"llama3\"\n",
        "result = query_model(\"What do Llamas eat?\", model)\n",
        "print(result)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WdF2iaRnD_9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in test_data[:3]:\n",
        "    prompt = (\n",
        "        f\"Given the input `{format_input(entry)}` \"\n",
        "        f\"and correct output `{entry['output']}`, \"\n",
        "        f\"score the model response `{entry['model_response']}`\"\n",
        "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "    )\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", query_model(prompt))\n",
        "    print(\"\\n-------------------------\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1m1ieuEsD_9t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the generated responses, we can observe that the Llama 3 model provides reasonable evaluations and is capable of assigning partial points when a model's answer is not entirely correct.\n",
        "\n",
        "The previous prompt returns highly detailed evaluations in addition to the score.\n",
        "\n",
        "We can modify the prompt to just generate integer scores ranging from 0 to 100, where 100 represents the best possible score.\n",
        "\n",
        "This modification allows us to calculate an average score for our model, which serves as a more concise and quantitative assessment of its performance.\n",
        "\n",
        "The following generate_model_scores function uses a modified the prompt telling the model to \"Respond with the integer number only.\":"
      ],
      "metadata": {
        "id": "5v9ScrWMD_9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entry in test_data[:2]:\n",
        "    prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry['model_response']}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "    score = query_model(prompt, model)\n",
        "    print(\"\\nDataset response:\")\n",
        "    print(\">>\", entry['output'])\n",
        "    print(\"\\nModel response:\")\n",
        "    print(\">>\", entry[\"model_response\"])\n",
        "    print(\"\\nScore:\")\n",
        "    print(\">>\", query_model(prompt, model))\n",
        "    print(\"\\n-------------------------\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "_MVj_11iD_9u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
        "    scores = []\n",
        "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
        "        prompt = (\n",
        "            f\"Given the input `{format_input(entry)}` \"\n",
        "            f\"and correct output `{entry['output']}`, \"\n",
        "            f\"score the model response `{entry[json_key]}`\"\n",
        "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
        "            f\"Respond with the integer number only.\"\n",
        "        )\n",
        "        score = query_model(prompt, model)\n",
        "        try:\n",
        "            scores.append(int(score))\n",
        "        except ValueError:\n",
        "            print(f\"Could not convert score: {score}\")\n",
        "            continue\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "trusted": true,
        "id": "5fbIczqiD_9v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To further improve our model's performance, we can explore various strategies, such as:\n",
        "\n",
        "(1) Adjusting the hyperparameters during finetuning, such as the learning rate, batch size, or number of epochs.\n",
        "\n",
        "(2) Increasing the size of the training dataset or diversifying the examples to cover a broader range of topics and styles.\n",
        "\n",
        "(3) Experimenting with different prompts or instruction formats to guide the model's responses more effectively.\n",
        "\n",
        "(4) Considering the use of a larger pretrained model, which may have greater capacity to capture complex patterns and generate more accurate responses.\n",
        "\n",
        "(5) We can also use parameter efficient fine-tuning techniques like LoRA."
      ],
      "metadata": {
        "id": "5XZEBoiVD_9v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "JVV8eCDMD_9w"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}