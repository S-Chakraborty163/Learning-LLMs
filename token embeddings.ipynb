{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d70223e-d4b8-4070-a68d-631fe1e28868",
   "metadata": {},
   "source": [
    "##### CREATING TOKEN EMBEDDINGs.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e6a115-782d-4825-bed7-af655d5fff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed90bd5-4039-47a4-a29e-2014dc1e6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb4d280-25d0-4444-827d-6ed59255ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3a1059-08c3-4351-830d-a6a3dcbc3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30e70677-35ee-4ab4-9471-262bce7e2f2c",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________________________________________________\n",
    "Just for simplicity, suppose we have a small vocab of only 6 words(instead of the 50,257 words in the BPE tokenizer vocab), and we want to create embeddings of size 3(in GPT-3, the embedding size is 12,288 dimensions)..\n",
    "_____________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8eb6372-3821-4735-a570-bb9604f44b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ff391-3cb5-4bad-8ae0-b34797a4410c",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________________\n",
    "class torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, _freeze=False, device=None, dtype=None)\n",
    "\n",
    "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "\n",
    "This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbe3088a-90a1-4125-be1b-7dd1e168cdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038867c-b47b-461c-97c0-77ff773ef864",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________________________________________________\n",
    "We can see that the weight matrix of the embedding layer contains small, random values.These values are optimized during LLM training as part of the LLM optimization itself.\n",
    "\n",
    "Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tockens in the vocab. And there is one column for each of the three embedding dimensions..\n",
    "_____________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50954552-104c-466d-9ac9-f64e90a0ef31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97458c-5b3c-4ff1-90a0-67c702b10db1",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________________________________________________\n",
    "The embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID..\n",
    "_____________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7dbff20-023c-43ab-bbbd-e2a5dbb759e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780e533-b1ec-4a1f-b8a5-f93d10c5e584",
   "metadata": {},
   "source": [
    "_____________________________________________________________________________________________________________________________________________________\n",
    "Each row in the output matrix is obtained via a lookup operation from the embedding weight matrix..\n",
    "_____________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb5df1-e565-4863-b0e3-0fa53721fab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edce92e-a339-4c7d-b173-1bbf9b0f88ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412a236f-fe45-4950-b84c-228ff15bd6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda2239-923b-4a81-b0a9-7c722671076f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac357f8-5386-4fd6-9af4-086cfe3f7c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fa27b-5888-4fc9-8260-cc5b73f00e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
